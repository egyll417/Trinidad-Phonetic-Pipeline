{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a07b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### A. Updating installers ###\n",
    "!pip install --upgrade pip setuptools wheel\n",
    "\n",
    "###B. Installing spaCy and BeautifulSoup ###\n",
    "!pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org beautifulsoup4 requests spacy --only-binary :all:\n",
    "\n",
    "### C. Downloading the English NLP data ###\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b1469d",
   "metadata": {},
   "source": [
    "# Automated Sociolinguistic Metadata Extraction (Trinidadian Creole)\n",
    "\n",
    "Research Context\n",
    "\n",
    "This file serves as a contextual mining tool for the broader study of Trinidadian rhoticity. While acoustic analysis provides quantitative data on phonological shift, this NLP-driven pipeline automatically extracts sociolinguistic metadata from research literature (Wikipedia, archives, etc.) to provide a historical and cultural framework for the speaker linguistic environment.\n",
    "\n",
    "Key Features:\n",
    "- Web Scraping: Automated retrieval of research text using Requests and BeautifulSoup.\n",
    "\n",
    "- NLP Metadata Extraction: Utilizing spaCy's Named Entity Recognition (NER) to identify ethnic groups (NORP), parent languages (LANGUAGE), and geographical hubs (GPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7130c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data from: https://en.wikipedia.org/wiki/Trinidadian_Creole...\n",
      "\n",
      "--- Automated Research Metadata Extraction ---\n",
      "               Entity  Category\n",
      "0         Trinidadian      NORP\n",
      "1            Trinidad       GPE\n",
      "2              Tobago       GPE\n",
      "3    Lesser Antillean      NORP\n",
      "4             English  LANGUAGE\n",
      "5          Tobagonian      NORP\n",
      "6              French      NORP\n",
      "7             African      NORP\n",
      "8         East Indian      NORP\n",
      "9          Amerindian      NORP\n",
      "10            Spanish      NORP\n",
      "11  Caribbean English      NORP\n",
      "12              China       GPE\n",
      "13         Portuguese      NORP\n",
      "14          Venezuela       GPE\n",
      "15            Madeira       GPE\n",
      "16              India       GPE\n",
      "17        west Africa       GPE\n",
      "18              Syria       GPE\n",
      "19            Lebanon       GPE\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "### 1. Loading spaCy ###\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def linguistic_literature_miner(url):\n",
    "    \"\"\"\n",
    "    Automated tool to scrape research text and extract \n",
    "    Geographic (GPE) and Institutional (ORG) metadata.\n",
    "    \"\"\"\n",
    "    # 1a. Setting headers to bypass block\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    # 1b. Scraping the content\n",
    "    print(f\"Scraping data from: {url}...\")\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 1c. Extracting only paragraph text\n",
    "    text = \" \".join([p.get_text() for p in soup.find_all('p')])\n",
    "    \n",
    "    # 1d. NLP Processing (NER), processing first 15,000 characters\n",
    "    doc = nlp(text[:15000]) \n",
    "    \n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['GPE', 'NORP', 'LANGUAGE']:\n",
    "            entities.append({'Entity': ent.text, 'Category': ent.label_})\n",
    "    \n",
    "    # 1e. Returning clean table\n",
    "    return pd.DataFrame(entities).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "### 2. Running the miner ###\n",
    "df_metadata = linguistic_literature_miner(\"https://en.wikipedia.org/wiki/Trinidadian_Creole\")\n",
    "\n",
    "print(\"\\n--- Automated Research Metadata Extraction ---\")\n",
    "print(df_metadata.head(20))\n",
    "\n",
    "### 3. Saving results to portfolio ###\n",
    "df_metadata.to_csv('linguistic_metadata_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e8d8e",
   "metadata": {},
   "source": [
    "### Data Synthesis & Observations\n",
    "By programmatically \"reading\" the academic summary of Trinidadian Creole, the model has identified several key vectors of influence:\n",
    "\n",
    "Language Contact: The extraction of LANGUAGE entities identifies the superstrate and substrate influences (English, French, Spanish) that compete with native Creole features.\n",
    "\n",
    "Cultural Stratigraphy: The NORP (Nationalities/Religious/Political) labels highlight the diverse demographic history (e.g., African, Indian, European) which directly correlates with the \"mixed-methods\" nature of the phonetic variation observed in our acoustic pipeline.\n",
    "\n",
    "Geographic Anchoring: Identifying GPE (Geopolitical Entities) allows for a quick mapping of the hub areas where language contact is most intense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f7945",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
